{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generative methods vs Discriminative methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "dataset = pd.read_csv('breast-cancer-wisconsin.data', header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      int64\n",
      "1      int64\n",
      "2      int64\n",
      "3      int64\n",
      "4      int64\n",
      "5      int64\n",
      "6     object\n",
      "7      int64\n",
      "8      int64\n",
      "9      int64\n",
      "10     int64\n",
      "dtype: object\n",
      "missing_num = 16\n",
      "missing_rate = 0.022889842632331903\n"
     ]
    }
   ],
   "source": [
    "# data pre-processing\n",
    "# take a look of the dataset, we can find the scalar of the first feature is so huge,so its better to do data pre-processing\n",
    "print(dataset.dtypes)\n",
    "# we find there are two col values are objects. we need to transform them into numbers\n",
    "# after checking the dataset, we can find there are some missing data in col7\n",
    "\n",
    "def find_missingdatarow_number(dataset):\n",
    "    n = 0\n",
    "    for i in range(data_num):\n",
    "        if dataset.loc[i][6] == '?':\n",
    "            n +=1 \n",
    "    return n\n",
    "data_num = dataset.shape[0]\n",
    "missing_num = find_missingdatarow_number(dataset)\n",
    "print(\"missing_num =\",missing_num)\n",
    "print(\"missing_rate =\",missing_num/data_num)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          0   1   2   3   4   5   6   7   8   9   10\n",
      "0    1000025   5   1   1   1   2   1   3   1   1   2\n",
      "1    1002945   5   4   4   5   7  10   3   2   1   2\n",
      "2    1015425   3   1   1   1   2   2   3   1   1   2\n",
      "3    1016277   6   8   8   1   3   4   3   7   1   2\n",
      "4    1017023   4   1   1   3   2   1   3   1   1   2\n",
      "..       ...  ..  ..  ..  ..  ..  ..  ..  ..  ..  ..\n",
      "678   776715   3   1   1   1   3   2   1   1   1   2\n",
      "679   841769   2   1   1   1   2   1   1   1   1   2\n",
      "680   888820   5  10  10   3   7   3   8  10   2   4\n",
      "681   897471   4   8   6   4   3   4  10   6   1   4\n",
      "682   897471   4   8   8   5   4   5  10   4   1   4\n",
      "\n",
      "[683 rows x 11 columns]\n"
     ]
    }
   ],
   "source": [
    "# the missing rows rate is very low. so we can just delete the rows with missing values\n",
    "def delete_missingdatarow_number(dataset):\n",
    "    for i in range(data_num):\n",
    "        if dataset.loc[i][6] == '?':\n",
    "            dataset = dataset.drop(index = [i])\n",
    "    return dataset\n",
    "dt = delete_missingdatarow_number(dataset)\n",
    "dt = dt.reset_index(drop = True)\n",
    "print(dt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    int64\n",
      "1    int64\n",
      "2    int64\n",
      "3    int64\n",
      "4    int64\n",
      "5    int64\n",
      "6    int64\n",
      "7    int64\n",
      "8    int64\n",
      "9    int64\n",
      "dtype: object\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-31-fa757c8bd823>:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X_dt[6] = X_dt[6].astype(int)\n"
     ]
    }
   ],
   "source": [
    "X_dt = dt[[0,1,2,3,4,5,6,7,8,9]]\n",
    "X_dt[6] = X_dt[6].astype(int)\n",
    "print(X_dt.dtypes)\n",
    "Y_dt = dt[10]\n",
    "Y_dt = Y_dt.astype(int)\n",
    "\n",
    "\n",
    "# Standrdlize\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "scaler = StandardScaler()\n",
    "X_dt = scaler.fit_transform(X_dt)\n",
    "from sklearn.model_selection import train_test_split\n",
    "train_X,test_X,train_y,test_y = train_test_split(X_dt,Y_dt, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_X.shape: (546, 10)\n",
      "test_X.shape: (137, 10)\n",
      "train_y.shape: (546,)\n",
      "test_y.shape: (137,)\n"
     ]
    }
   ],
   "source": [
    "print(\"train_X.shape:\", train_X.shape)\n",
    "print(\"test_X.shape:\", test_X.shape)\n",
    "print(\"train_y.shape:\", train_y.shape)\n",
    "print(\"test_y.shape:\", test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-proccessing Finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add one col as bias term\n",
    "ones_train = np.ones((train_X.shape[0], 1))\n",
    "new_train_X = np.hstack((train_X,ones_train))\n",
    "ones_test = np.ones((test_X.shape[0], 1))\n",
    "new_test_X = np.hstack((test_X,ones_test))\n",
    "\n",
    "x_tra = pd.DataFrame(new_train_X)\n",
    "x_tes = pd.DataFrame(new_test_X)\n",
    "y_tra = train_y.reset_index(drop = True)\n",
    "y_tes = test_y.reset_index(drop = True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold\n",
    "from sklearn.model_selection import KFold\n",
    "kf = KFold(n_splits=5,shuffle=True)\n",
    "\n",
    "def log_likelihood(x, y, w):\n",
    "    xw = np.dot(x, w)\n",
    "    likelihood =  np.sum(y*xw - np.log(1 + np.exp(xw)))\n",
    "    return likelihood\n",
    "\n",
    "def sigmoid(a):\n",
    "    return 1 / (1 + np.exp(-a))\n",
    "\n",
    "def SGD(tra_x,tra_y, epoch_n, lr):     \n",
    "    w = np.random.rand(tra_x.shape[1])  # 11 x 1\n",
    "    train_sample_num = tra_x.shape[0]\n",
    "    feature_num = tra_x.shape[1] \n",
    "    objval = 0\n",
    "    for step in range(epoch_n):\n",
    "        rand_indices = np.random.permutation(train_sample_num)\n",
    "        tra_x_rand = tra_x.values[rand_indices, :]\n",
    "        tra_y_rand = tra_y.values[rand_indices]\n",
    "        for i in range(train_sample_num):\n",
    "            xi = tra_x_rand[i,:]\n",
    "            yi = tra_y_rand[i]\n",
    "            predict = sigmoid(np.dot(xi, w))                \n",
    "            y_minus_predict = yi - predict\n",
    "            gradient = np.dot(xi.T, y_minus_predict)\n",
    "            w += lr * gradient\n",
    "            ll = log_likelihood(xi, yi, w)   \n",
    "            objval += ll \n",
    "        objval = objval/train_sample_num\n",
    "    print('log_likelihood_SGD after 50 epoch is', str(objval))\n",
    "    return w\n",
    "\n",
    "def Mini_batch_GD(tra_x,tra_y, epoch_n, lr,batch_size):\n",
    "    w = np.random.rand(tra_x.shape[1])  # 11 x 1\n",
    "    train_sample_num = tra_x.shape[0]\n",
    "    feature_num = tra_x.shape[1] \n",
    "    objval = 0\n",
    "    for step in range(epoch_n):\n",
    "        rand_indices = np.random.permutation(train_sample_num)\n",
    "        tra_x_rand = tra_x.values[rand_indices, :]\n",
    "        tra_y_rand = tra_y.values[rand_indices]\n",
    "        for i in range(0,train_sample_num,batch_size):\n",
    "            xi = tra_x_rand[i:i+batch_size,:]\n",
    "            yi = tra_y_rand[i:i+batch_size]\n",
    "            predict = sigmoid(np.dot(xi, w))                \n",
    "            y_minus_predict = yi - predict\n",
    "            gradient = np.dot(xi.T, y_minus_predict)\n",
    "            w += lr * gradient\n",
    "            ll = log_likelihood(xi, yi, w)   \n",
    "            objval += ll \n",
    "        objval = objval/train_sample_num\n",
    "    print('log_likelihood_Minibatch_GD after 50 epoch is', str(objval))\n",
    "    return w          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "log_likelihood_SGD after 50 epoch is -0.06934480134371307\n",
      "log_likelihood_Minibatch_GD after 50 epoch is -0.07014106328575777\n",
      "log_likelihood_SGD after 50 epoch is -0.0699295719211398\n",
      "log_likelihood_Minibatch_GD after 50 epoch is -0.0702587072600456\n",
      "log_likelihood_SGD after 50 epoch is -0.04800054339285878\n",
      "log_likelihood_Minibatch_GD after 50 epoch is -0.0487559925426709\n",
      "log_likelihood_SGD after 50 epoch is -0.07533928636551158\n",
      "log_likelihood_Minibatch_GD after 50 epoch is -0.07468737256544962\n",
      "log_likelihood_SGD after 50 epoch is -0.06748093514868099\n",
      "log_likelihood_Minibatch_GD after 50 epoch is -0.06789398763255682\n",
      "log_likelihood with SGD is: -10.888875638389559\n",
      "log_likelihood with Minibatch_gd is: -10.88935396537002\n"
     ]
    }
   ],
   "source": [
    "epoch_n = 50\n",
    "lr = 0.05\n",
    "w_folds_total_sgd = 0\n",
    "w_folds_total_mini = 0\n",
    "total_ll_sgd = 0\n",
    "total_ll_mini = 0\n",
    "\n",
    "# cross-validation\n",
    "for train_index, val_index in kf.split(x_tra):\n",
    "        tra_x,val_x = x_tra.loc[train_index],x_tra.loc[val_index]\n",
    "        tra_y,val_y = y_tra[train_index],y_tra[val_index] \n",
    "        tra_y = (tra_y)/2 - 1\n",
    "        val_y = (val_y)/2 - 1\n",
    "#       perform SGD and get the w\n",
    "        w_sgd = SGD(tra_x,tra_y, epoch_n, lr)\n",
    "#       perform minibatch_GD and get the w\n",
    "        w_minibatchgd = Mini_batch_GD(tra_x,tra_y, epoch_n,lr,batch_size = 20)    \n",
    "        likelihood_on_valdata_sgd = log_likelihood(val_x,val_y,w_sgd)\n",
    "        likelihood_on_valdata_mini = log_likelihood(val_x,val_y,w_minibatchgd)\n",
    "        total_ll_sgd += likelihood_on_valdata_sgd\n",
    "        total_ll_mini += likelihood_on_valdata_mini     \n",
    "        w_folds_total_sgd += w_sgd\n",
    "        w_folds_total_mini += w_minibatchgd\n",
    "# avrage the w and loss function result\n",
    "w_star_sgd = w_folds_total_sgd/5\n",
    "w_star_mini = w_folds_total_mini/5\n",
    "total_ll_sgd = total_ll_sgd/5\n",
    "total_ll_mini = total_ll_mini/5\n",
    "\n",
    "print(\"log_likelihood with SGD is:\",total_ll_sgd)\n",
    "print(\"log_likelihood with Minibatch_gd is:\",total_ll_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from the results, we can see SGD and Minibatch_Gd works perfect on our model with hyperparameter lr= 0.05,epoch = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "137\n",
      "0      0.0\n",
      "1      0.0\n",
      "2      0.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "      ... \n",
      "132    0.0\n",
      "133    0.0\n",
      "134    1.0\n",
      "135    1.0\n",
      "136    0.0\n",
      "Length: 137, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# recall, precision, and accuracy on (1) class prediction on the test data\n",
    "# In this model, SGD and Mini Batch GD performs same well, so i'm gonna use SGD to do the test .\n",
    "y_predicted = sigmoid(x_tes.dot(w_star_sgd))\n",
    "test_len = y_predicted.shape[0]\n",
    "print(test_len)\n",
    "for i in range(test_len):\n",
    "    if y_predicted[i] < 0.5:\n",
    "        y_predicted[i] = 0\n",
    "    else:\n",
    "        y_predicted[i] = 1\n",
    "print(y_predicted)\n",
    "            \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0      0.0\n",
      "1      0.0\n",
      "2      0.0\n",
      "3      0.0\n",
      "4      0.0\n",
      "      ... \n",
      "132    0.0\n",
      "133    0.0\n",
      "134    1.0\n",
      "135    1.0\n",
      "136    0.0\n",
      "Name: 10, Length: 137, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# cuz in original dataset, the label is either 2 or 4 but we need o and 1.\n",
    "y_tes = y_tes/2 - 1\n",
    "print(y_tes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "right_class1 : 81\n",
      "right_class2 : 49\n",
      "wrong_class1 : 5\n",
      "wrong_class2 : 2\n"
     ]
    }
   ],
   "source": [
    "right_class1 = 0\n",
    "right_class2 = 0\n",
    "wrong_class1 = 0\n",
    "wrong_class2 = 0\n",
    "for i in range(test_len):\n",
    "    if y_predicted[i] == y_tes[i] and y_predicted[i] == 0 :\n",
    "        right_class1+=1\n",
    "    if y_predicted[i] == y_tes[i] and y_predicted[i] == 1 :\n",
    "        right_class2+=1\n",
    "    if y_predicted[i] == 0 and y_tes[i] == 1 :\n",
    "        wrong_class1+=1\n",
    "    if y_predicted[i] == 1 and y_tes[i] == 0 :\n",
    "        wrong_class2+=1\n",
    "print(\"right_class1 :\",right_class1)\n",
    "print(\"right_class2 :\",right_class2)\n",
    "print(\"wrong_class1 :\",wrong_class1)\n",
    "print(\"wrong_class2 :\",wrong_class2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "precision for malignant on testset : 0.9607843137254902\n",
      "recall for malignant on testset : 0.9074074074074074\n",
      "accuracy for the model on testset : 0.948905109489051\n"
     ]
    }
   ],
   "source": [
    "# For malignant class\n",
    "precision = right_class2 / (right_class2+wrong_class2)\n",
    "recall = right_class2 / (right_class2+wrong_class1)\n",
    "accuracy = (right_class1+right_class2) / test_len\n",
    "print(\"precision for malignant on testset :\",precision)\n",
    "print(\"recall for malignant on testset :\",recall)\n",
    "print(\"accuracy for the model on testset :\",accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Model is not bad !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6rc1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
